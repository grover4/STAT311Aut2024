---
title: "Rate my Prof: Multiple Linear Regression"
subtitle: "STAT 311 Autumn 2024"
author: "Your Name Here"
date: "`r Sys.Date()`"
output:    
      html_document:
         toc: yes
         toc_float: yes
---


We return to the `evals` dataframe from the **openintro** package to fit more models to the course evaluation `score`. 

# Learning goals

-   Fitting linear models with two numerical and one categorical predictors

-   Interpreting coefficients of the linear model

-   Comparing models

Let's go ahead and load the packages we will need for the analysis.

## Packages

We will work with the **tidyverse**, **tidymodels** and the **openintro** packages as before. We will also use the `**GGally** package for making pairwise scatter plots.


```{r setup, include = FALSE}
#load packages and set global options 

library(tidyverse)     #umbrella package 
library(openintro)     #for the data
library(tidymodels)    #for model fitting/model comparison   
library(GGally)        #pairwise scatterplots

```



## Exercises 

### Part III: Main effects with multiple predictors

Let's return to the `evals` data in the **openintro** package and fit a few additional models for predicting the course evaluation ratings. Our focus is on interpreting the model equations. The key thing to note is that model building is a process, and you should be willing to explore and try different things. 

The explanatory variables in this data set are either numerical or categorical and are of two types as shown below.

 - prof characteristics: `rank`, `ethnicity`, `gender`, `age`,  `bty_avg`
 
 - class characteristics: `cls_perc_eval`,  `cls_profs`, `cls_level`, `cls_credits`
 
A reasonable strategy for manually building a model to predict course evaluation `scores` is to pick a couple of numerical predictors and one categorical predictor. 

In this part, we will examine the numerical predictors first to identify a couple that seem relevant. Then we will turn to finding a categorical predictor.

### Exercise 1

Use `ggpairs` to make a scatter plot matrix of `score` versus the numeric predictors:  `age`, `bty_avg`, `cls_perc_eval`. 

```{r}
#correlation heat map
#mycorr <- evals %>% select(age, bty_avg, cls_students, cls_perc_eval, score) %>% cor(use="complete.obs")

#corrplot(mycorr)


evals %>% 
  select(age, bty_avg, cls_perc_eval,score) %>% 
  ggpairs(lower=list(continuous = "smooth"))


```


The scatterplots don't show very strong relationships. Let's just pick `bty_avg` and `cls_perc_eval` as our numeric variables. They have slightly higher correlations with score (0.18) compared to age (-0.11). Also, `age` is somewhat correlated with `bty_avg`, so there is no need to have both in the model.

### Exercise 2

Fit a main effects model `model1` to predict `score` from `bty_avg` and `cls_perc_eval`. 

   - Present the output in a tidy format and write the equation of the fitted model.
   
   - Assess goodness of fit by calculating the adjusted R-square. Save this number in a variable called `model1_rsq` and report it within a complete sentence using inline code.
   
```{r}

model1 <- lm(score ~ cls_perc_eval + bty_avg, data = evals)

model1 %>% tidy()

model1_rsq <- glance(model1) %>% 
              select(adj.r.squared) %>% pull()

```
   
   
Now, let us turn our attention to adding a categorical predictor into our model equation.

### Exercise 3

Make a visualization of `score` versus  `cls_level`. (You can also calculate grouped numerical summaries of `score` by `cls_level`)

```{r}

evals %>% select(rank, score) %>% ggpairs()

evals %>% select(gender, score) %>% ggpairs()

evals %>% select(cls_level, score) %>% ggpairs()

evals %>% group_by(gender) %>% summarise(median_scr = median(score))

evals %>% group_by(cls_level) %>% summarise(median_scr = median(score))

evals %>% group_by(rank) %>% summarise(median_scr = median(score))

```

### Exercise 4

Fit a main effects model `model2` to predict  `score` from `bty_avg`, `cls_perc_eval` and `cls_level`. 

     - Present the output in a tidy format and write the equation of the fitted model.
   
    - Assess goodness of fit by calculating the adjusted R-square. Save this number in a variable called `model2_rsq` and report it within a complete sentence using inline code.
   

```{r}

model2 <- lm(score ~ bty_avg + cls_perc_eval + cls_level, data = evals)

model2 %>% tidy()

model2_rsq <- glance(model2) %>% 
              select(adj.r.squared) %>% pull()
```







Let us now add the adjusted R-suare (as a percentage and rounded to 2 digits) for `model1` and `model2`in the table below for easy comparison.

| Model     | Adjusted R square
|:----      | :------------    
| `model1`  | `r round(100 * model1_rsq,2)`
| `model2`  | `r round(100 * model2_rsq,2)`



***OPTIONAL: For those who want to stretch***


Let us now consider `rank` as a possible predictor of `score`. Notice that it has three levels: teaching, tenured and tenure track.

5. Make a visualization `score` versus  `rank`. (You can also calculate grouped numerical summaries of `score` by `rank``)


6. Fit a main effects model `model3` to predict  `score` from `bty_avg`, and `rank`. 

     - Present the output in a tidy format and write the equation of the fitted model.
   
    - Assess goodness of fit by calculating the adjusted R-square. Save this in a variable called `model3_rsq` and report it within a complete sentence using inline code.
    

```{r}

model3 <- lm(score ~ bty_avg + rank, data = evals)

model3 %>% tidy()

glance(model3) %>% select(adj.r.squared)

```


The fitted model equation is score-hat = 3.98 + 0.068 * bty_avg - 0.16*ranktenure track - 0.126 ranktenured. 

Holding all other variables fixed,when bty_avg increases by 1 the score is expected to increase by 0.068 on average. 

When comparing two instructors with the same bty_avg, one in a teaching track and the other in tenure track, the latter is expected to score 0.16 lower.

When comparing two instructors with the same bty_avg, one in a teaching track and the other tenured, the latter is expected to score 0.126 lower.






## Acknowledgment

This activity is from Data Science in a Box